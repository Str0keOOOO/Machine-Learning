# 所有机器学习适用情况

以轴承故障诊断为例包含了机器学习的大部分知识

- [逻辑回归](##逻辑回归)
- [贝叶斯](##贝叶斯)
- [K近邻](##K近邻)
- [决策树](##决策树)
- [随机森林](##随机森林)
- [支持向量机](##支持向量机)
- [XGBoost](##[XGBoost)
- [LightGBM](##LightGBM)
- [BP神经网络](##[BP神经网络)
- [线性判别](##线性判别)

$$有监督学习\begin{cases}
决策树\begin{cases}
ID3\\
C4.5(+Bagging)\rightarrow 随机森林\\
CART(+Boosting)\rightarrow Adaboost,GBDT\\
\end{cases}\\
线性模型\begin{cases}
回归\rightarrow 线性回归\begin{cases}
岭回归\\
LASSQ回归
\end{cases}\\
分类\rightarrow 感知器\begin{cases}
SVM\\
Logistic回归\rightarrow softmax回归\\
ANN/MLP\begin{cases}
CNN\rightarrow FCN\\
AE,RBM\rightarrow DEM,DBN\\
RNN/LSTM\\
GAN\\
\end{cases}\\
\end{cases}\\
\end{cases}\\
KNN\ 距离度量练习\\
Bates\begin{cases}
朴素贝叶斯\\
贝叶斯网络\\
正态贝叶斯\\
\end{cases}\\
LDA\rightarrow KLDA
\end{cases}\\
无监督学习\begin{cases}
降维\begin{cases}
PCA\rightarrow KPCA\\
流形学习\begin{cases}
LLE\\
拉普拉斯特征映射\\
等距映射\\
局部保持投影\\
\end{cases}\\
\end{cases}\\
聚类\begin{cases}
层次聚类\\
k-means\\
DBSCAN\\
OPTICS\\
Mean shift\\
谱聚类\\
EM算法\\
\end{cases}\\
\end{cases}\\
强化学习\begin{cases}
策略迭代\\
价值迭代\\
蒙特卡洛算法\\
时序差分算法\begin{cases}
SARSA算法\\
Q学习\rightarrow DQN,策略纬度\\
\end{cases}\\
\end{cases}\\
$$

## 逻辑回归

LR 是很多分类算法的基础组件，它的好处是输出值自然地落在 0 到 1 之间，并且有概率意义。因为 LR 本质上是一个线性的分类器，所以处理不好特征之间相关的情况。

虽然预测效果一般，但胜在模型清洗，背后的概率学经得起推敲。它拟合出来的参数就代表了每一个特征对结果的影响。也是一个理解数据的好工具。

**使用条件：**当数据线性可分，特征空间不是很大的情况，不用在意新数据的情况，后续会有大量新数据的情况。

**小结：**

- 用于分类：适合做很多分类算法的基础组件。
- 用于预测：预测事件发生的概率（输出）。
- 用于分析：单一因素对某一个事件发生的影响因素分析（特征参数值）。

**优点：**

- 从整体模型来说，模型清洗，背后的概率推导经得住推敲；
- 从输出值来说，输出值自然落在 0 到 1 之间，并且有概率意义；
- 从模型参数来说，参数代表每个特征对输出的影响，可解释性强；
- 从运行速度来说，实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用；
- 从过拟合角度来说，解决过拟合的方法很多，如 L1、L2正则化；
- 从多重共线性来说，L2 正则化就可以解决多重共线性问题；

**缺点：**

- （特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况；
- （特征空间）特征空间很大时，性能不好；
- （预测精度）容易欠拟合，预测精度不高；

## 贝叶斯

**朴素贝叶斯算法注意点：**

- 当特征属性值的值类型不是离散值而是连续值的时候，需要通过高斯分布做概率的计算；
- 为了避免统计概率中出现概率为0的情况，可以引入Laplace校准，它的思想非常简单，就是对没类别下所有划分的计数加1。

**适用场景及主要应用领域：**

1. 朴素贝叶斯算法对待预测样本进行预测，过程简单速度快；
2. 对于多分类问题也同样很有效，复杂度也不会有大程度上升；
3. 在分布独立这个假设成立的情况下，贝叶斯分类器效果奇好，会略胜于逻辑回归，同时需要的样本量也更少一点。
4. 对于类别类的输入特征变量，效果非常好。对于数值型变量特征，我们是默认它符合正态分布的。

**主要应用领域**

- 文本分类/垃圾文本过滤/情感判别：多分类较为简单，同时在文本数据中，分布独立这个假设基本是成立的。垃圾文本过滤(比如垃圾邮件识别)和情感分析(微博上的褒贬情绪)用朴素贝叶斯也通常能取得很好的效果。
- 多分类实时预测：对于文本相关的多分类实时预测，朴素贝叶斯算法被广泛应用，简单又高效。
- 推荐系统：朴素贝叶斯和协同过滤(Collaborative Filtering)是一对好搭档，协同过滤是强相关性，但是泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。

## K近邻

**K近邻（KNN）算法优点：**

- 算法原理简单，容易理解，也较容易实现。
- 不需要进行训练，只需要保存训练样本和标签。
- 不易受小错误概率的影响。经理论证明，最近邻的渐进错误率最坏时不超过两倍的贝叶斯错误率，最好时接近或达到贝叶斯错误率。

**K近邻（KNN）算法缺点：**

- K的选择不固定。
- 预测结果容易受到噪声数据的影响。
- 当样本不平衡时，新样本的类别偏向训练样本中数量占优的类别，容易导致预测错误。
- 当数据量较大时，具有较高的计算复杂度和内存消耗，因为对每一个待分类的文本，都要计算它到全体已知样本的距离，才能求得它的K个最近邻。

**适用场景及主要应用领域：**

由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。在实际应用当中，KNN算法在人脸识别、文字识别、医学图像处理等领域可以取得良好的分类效果。

**K近邻（KNN）算法需要注意的问题：**

- 数据特征之间量纲不统一时，需要对数据进行归一化处理，否则会出现大数吃小数的问题；
- 数据之间的距离计算通常采用欧式距离；
- KNN算法中K值的选取会对结果产生较大的影响，一般k值要小于训练样本数据的平方根；
- 通常采用交叉验证法来选择最优的K值。

## 决策树

**1、理解树模型**

决策树模型是运用于分类以及回归的一种树结构。决策树由节点和有向边组成，一般一棵树包含一个根节点、若干内部节点和若干叶节点。决策树的决策过程需要从根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择下一比较分支，直至叶节点作为最终的决策结果

**2、树模型的应用**

树模型可以生成清晰的基于特征选择不同预测结构的树状结构，当你希望可以更好的理解手上的数据的时候往往可以使用决策树。同时它也是相对容易被供给的分类器（因为这里认为的改变一些特征，是的分类器判断错误。常见于垃圾邮件躲避检测中，因为决策树最终在底层的判断是基于单个条件，攻击者往往只需要改变很少的特征就可以躲过监测）。

**小结：**

- 受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石；

**3、树模型的优缺点**

**优点：**

- 容易理解、可读性强，比较直观；
- 自变量/特征可以是连续变量，也可以是分类变量；
- 可处理缺失值；
- 基本不用做原始数据的预处理，如标准化等；
- 可以建立非线性模型；
- 即使是较大的数据及，其训练时间也很短；

**劣势：**

- 大型的决策树较难解释；
- 方差大的决策树会导致模型表现较差；
- 容易出现过拟合；

## 随机森林

随机森林是一种集成算法。它首先随机选取不同的特征和训练样本，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。

随机森林在现实分析中被大量使用，相对于决策树，在准确性有了很大的额提升，同时一定程度上改善了决策树容易被攻击的特点。

**随机森林的应用：**

- 当数据维度相对低（几十维），同时对准确性有较高要求时；
- 因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候可以先试一下随机森林；

**随机森林的优缺点：**

**优点：**

- 在所有的算法中，随机森林具有极好的准确率；
- 能够运行在大数据集上；
- 能够处理具有高维特征的输入样本，而且不需要降维；
- 能够评估各个特征在分类问题上的重要性；
- 在生成过程中，能够获取内部生成误差的一种无偏估计；
- 对于缺失值也能够获得很好的结果；

**缺点：**

- 据观测，如果一些分类/回归问题的训练数据中存在噪音，随机森林中的数据集中会出现过拟合的现象；
- 相对决策树来说，随机森林算法更复杂，计算成本更高（因为 RF 是有多个决策树组成）；
- 由于其本身的复杂性，它们比其他类似的算法需要更多的时间来训练；

## 支持向量机

支持向量机，support vector machine，简称SVM，是经典机器学习的一个重要分类算法，用于完成数据分类。svm算法通过找出一个决策超平面（二维空间指直线，三维空间指平面，超过三维的就是超平面了），将已有训练数据集划分开，然后对于新数据，根据数据是位于超平面的哪一侧完成判断，得到新数据的分类。

**SVM算法注意点：**

- SVM可以执行线性非线性的分类、回归、异常值检测。适用于中小型复杂数据集；
- SVM对特征缩放很敏感，可以在输入之前采用StandardScaler处理；
- 如果SVM模型过拟合，可以通过降低C来进行正则化；
- LinearSVC灰度偏执正则化，需要减去平均值，StandardScaler会自动处理；
- 处理非线性数据集的方法之一是添加更多的特征，比如多项式特征。

**适用场景及主要应用领域：**

支持向量机(Support Vector Machine)是Cortes和Vapnik于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，但它具有以下缺点：

- 无法应对大规模训练样本；
- 难以解决多分类问题；
- 对参数及核函数选择非常敏感。

支持向量机的常见适用范围如下：

**1. 网络完全**

传统的网络入侵检测方法大多采用密码签名的方法。在进行入侵检测方面，机器学习技术可以帮助我们进行网络流量的分析，在这里支持向量机具有检测速度快，分类精度高等特点，可以帮助安全人员识别不同类别的网络攻击，例如扫描和欺诈网络。

**2. 人脸识别**

SVM可以将图像部分分为人脸和非人脸。它包含nxn像素的训练数据，具有两类人脸（+1）和非人脸（-1），然后从每个像素中提取特征作为人脸和非人脸。根据像素亮度在人脸周围创建边界，并使用相同的过程对每个图像进行分类。

**3. 文本和超文本分类**

SVM可以实现对两种类型的模型进行文本和超文本分类，它主要通过使用训练数据将文档分类为不同的类别，如新闻文章、电子邮件和网页。

对于每个文档，计算一个分数并将其与预定义的阈值进行比较。当文档的分数超过阈值时，则将文档分类为确定的类别。如果它不超过阈值，则将其视为一般文档。

通过计算每个文档的分数并将其与学习的阈值进行比较来对新实例进行分类。

**4. 蛋白质折叠和远程同源检测**

蛋白质远程同源性检测是计算生物学中的一个关键问题。SVM算法是远程同源检测最有效的方法之一。这些方法的性能取决于蛋白质序列的建模方式。

## XGBoost

XGBoost是一种基于决策树的集成学习算法。其核心思想是通过迭代地训练一系列决策树，并将它们组合起来以获得更好的预测性能。每棵树都试图纠正前一棵树的错误，从而逐渐提高模型的精度。

**优点：**

- 高效：XGBoost采用了许多优化技巧，使其在训练和预测阶段都非常高效；
- 灵活：XGBoost可以处理各种类型的数据集，包括连续和离散特征；
- 可解释性强：由于是基于决策树的集成学习算法，XGBoost的模型结构相对直观，易于解释。

**缺点**

- 对参数敏感：XGBoost的性能对超参数设置非常敏感，需要仔细调整；
- 对数据预处理要求高：为了获得最佳性能，需要对数据进行适当的预处理，例如特征缩放和编码。

## LightGBM

**优点：**简单易用。提供了主流的Python\C++\R语言接口，用户可以轻松使用LightGBM建模并获得相当不错的效果。高效可扩展。在处理大规模数据集时高效迅速、高准确度，对内存等硬件资源要求不高。鲁棒性强。相较于深度学习模型不需要精细调参便能取得近似的效果。
**缺点：**直接支持缺失值与类别特征，无需对数据额外进行特殊处理,相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先LightGBM。

## BP神经网络

**适用条件：**数据量庞大，参数之间存在内在联系。

**优点：**1.并行分布处理能力强；2.提取数据特征；3.逼近复杂的非线性关系。

**缺点：**1.需要大量参数；2.学习时间过长；3.不能观察之间的学习过程，输出结果难以解释。

## 线性判别

**优点：**

- 在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。
- LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。

**缺点：**

- LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。
- LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。
- LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。
- LDA可能过度拟合数据。
